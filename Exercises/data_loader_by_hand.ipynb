{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jakhongir0103/Pytorch_course_for_DL/blob/main/Exercises/data_loader_by_hand.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n9LOuQ5Qka1h",
        "outputId": "cf27e0e3-8981-49b1-ead2-4fba1369610c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "from torchvision.datasets import MNIST\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# get as dataloaders\n",
        "train_data = MNIST(root='mnist_data/train', train=True, download=True)\n",
        "test_data = MNIST(root='mnist_data/test', train=False, download=True)\n",
        "\n",
        "# convert to numpy\n",
        "train_data_X, train_data_y = train_data.data.detach().numpy(), train_data.targets.detach().numpy()\n",
        "test_data_X, test_data_y = test_data.data.detach().numpy(), test_data.targets.detach().numpy()\n",
        "\n",
        "# # normalize\n",
        "# train_data_X = train_data_X / 255\n",
        "# test_data_X = test_data_X / 255\n",
        "\n",
        "# print shapes\n",
        "train_data_X.shape, train_data_y.shape, test_data_X.shape, test_data_y.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "xVcg98ocnd-M",
        "outputId": "03915d11-f5e1-4ceb-c7f9-8dced454e57f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvKDNuxrAtwv"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TtKZL82hAsst"
      },
      "outputs": [],
      "source": [
        "class CNNClassifier(nn.Module):\n",
        "    def __init__(self, in_shape=1, out_shape=10, hidden_unit=10):\n",
        "        super().__init__()\n",
        "        self.convolution_block = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_shape, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(in_channels=hidden_unit, out_channels=hidden_unit, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2)\n",
        "        )\n",
        "        self.classifier_block = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=7*7*hidden_unit, out_features=out_shape)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        x = self.convolution_block(x)\n",
        "        x = self.classifier_block(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HfNYI7J72FmA"
      },
      "source": [
        "# Without data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRfjz8KXAx67"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifQpEeLykjim",
        "outputId": "b11e4c21-544b-408e-9d18-0a35059c1dd8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([60000, 1, 28, 28]), torch.Size([60000]))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# conver the train data to tensor\n",
        "train_data_X_torch = torch.from_numpy(train_data_X).to(torch.float32).unsqueeze(dim=1)\n",
        "train_data_y_torch = torch.from_numpy(train_data_y).to(torch.int64)\n",
        "\n",
        "# conver the test data to tensor\n",
        "test_data_X_torch = torch.from_numpy(test_data_X).to(torch.float32).unsqueeze(dim=1)\n",
        "test_data_y_torch = torch.from_numpy(test_data_y).to(torch.int64)\n",
        "\n",
        "# check the shape\n",
        "train_data_X_torch.shape, train_data_y_torch.shape # Batch x Dimension(3 for RGB) x height x width"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP9AsZ2IA0_d"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PLj8Fy7k565",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab97086c-fd4b-46c6-97f3-fa0b9dbb25da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss - 2.377725601196289\n",
            "Epoch 1: loss - 2.377725601196289\n",
            "Epoch 2: loss - 2.377725601196289\n",
            "Epoch 3: loss - 2.3777241706848145\n",
            "Epoch 4: loss - 2.3777239322662354\n",
            "Time 631.7624289989471\n"
          ]
        }
      ],
      "source": [
        "from time import time\n",
        "\n",
        "# define the model\n",
        "clf = CNNClassifier().to(device)\n",
        "\n",
        "# loss and optimization\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=clf.parameters(), lr=0.01)\n",
        "\n",
        "start = time()\n",
        "losses = []\n",
        "\n",
        "# train loop\n",
        "for epoch in range(5):\n",
        "  # train\n",
        "  clf.train()\n",
        "\n",
        "  for X, y in zip(train_data_X_torch, train_data_y_torch):\n",
        "    # prepare the data\n",
        "    X, y = X.to(device).unsqueeze(0), y.to(device).unsqueeze(0)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fn(y_logit, y)\n",
        "    losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    # update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # track loss\n",
        "  print(f\"Epoch {epoch}: loss - {np.mean(losses)}\")\n",
        "\n",
        "print(\"Time\",time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EONKGNon_Qg",
        "outputId": "21b0cad8-f2bd-4a41-a9e2-df7aa8ab275b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.0958\n"
          ]
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "  # predictions\n",
        "  prediction = []\n",
        "\n",
        "  # test\n",
        "  clf.eval()\n",
        "  for X, y in zip(test_data_X_torch, test_data_y_torch):\n",
        "    X, y = X.to(device).unsqueeze(0), y.to(device).unsqueeze(0)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "    prediction.append(torch.argmax(y_logit, dim=1).item())\n",
        "\n",
        "# get accuracy\n",
        "acc = accuracy_score(test_data_y, prediction)\n",
        "print(f\"Mean accuracy: {np.mean(acc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ71AJJ22LD6"
      },
      "source": [
        "# With data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_LDQvALA7vu"
      },
      "source": [
        "### Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "oMe2mAAS2J-H"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataLoader(Dataset):\n",
        "  def __init__(self, X, y, transform=None, target_transform=None) -> None:\n",
        "    self.data = X\n",
        "    self.target = y\n",
        "    self.transform = transform\n",
        "    self.target_transform = target_transform\n",
        "\n",
        "  def __getitem__(self, idx) -> None:\n",
        "    return self.data[idx], self.target[idx]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.target)\n",
        "\n",
        "train_dataset = CustomDataLoader(train_data_X, train_data_y, transforms.ToTensor())\n",
        "test_dataset = CustomDataLoader(test_data_X, test_data_y, transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbK4PBdrA860"
      },
      "source": [
        "### Train - batch size of 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9pXTHyR8-tG",
        "outputId": "a55db297-bdf8-489b-be19-134245ec111b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss - 2.377725124359131\n",
            "Epoch 1: loss - 2.377725124359131\n",
            "Epoch 2: loss - 2.377725124359131\n",
            "Epoch 3: loss - 2.37772536277771\n",
            "Epoch 4: loss - 2.377725601196289\n",
            "Time 692.4034841060638\n"
          ]
        }
      ],
      "source": [
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=1)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=1)\n",
        "\n",
        "# define the model\n",
        "clf = CNNClassifier().to(device)\n",
        "\n",
        "# loss and optimization\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=clf.parameters(), lr=0.01)\n",
        "\n",
        "start = time()\n",
        "losses = []\n",
        "\n",
        "# train loop\n",
        "for epoch in range(5):\n",
        "  # train\n",
        "  clf.train()\n",
        "  # iterate over the dataset\n",
        "  for X, y in train_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device).to(torch.float32).unsqueeze(dim=1)\n",
        "    y = y.to(device).to(torch.int64)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fn(y_logit, y)\n",
        "    losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    # update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # track loss\n",
        "  print(f\"Epoch {epoch}: loss - {np.mean(losses)}\")\n",
        "\n",
        "print(\"Time\",time()-start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbSGpwSb-Ntg",
        "outputId": "ee92c56c-226b-4b64-8fcc-5be59cc11e04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.0958\n"
          ]
        }
      ],
      "source": [
        "# evaluate - Batch size of 1 only\n",
        "with torch.inference_mode():\n",
        "  # predictions\n",
        "  prediction = []\n",
        "  # test\n",
        "  clf.eval()\n",
        "  for X, y in test_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device).to(torch.float32).unsqueeze(dim=1)\n",
        "    y = y.to(device).to(torch.int64)\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "    prediction.append(torch.argmax(y_logit, dim=1).item())\n",
        "\n",
        "# get accuracy\n",
        "acc = accuracy_score(test_data_y, prediction)\n",
        "print(f\"Mean accuracy: {np.mean(acc)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train - batch size of 32"
      ],
      "metadata": {
        "id": "7VIEl4Bhp5IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=32)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=32)\n",
        "\n",
        "# define the model\n",
        "clf = CNNClassifier().to(device)\n",
        "\n",
        "# loss and optimization\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=clf.parameters(), lr=0.01)\n",
        "\n",
        "start = time()\n",
        "losses = []\n",
        "\n",
        "# train loop\n",
        "for epoch in range(5):\n",
        "  # train\n",
        "  clf.train()\n",
        "  # iterate over the dataset\n",
        "  for X, y in train_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device).to(torch.float32).unsqueeze(dim=1)\n",
        "    y = y.to(device).to(torch.int64)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fn(y_logit, y)\n",
        "    losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    # update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # track loss\n",
        "  print(f\"Epoch {epoch}: loss - {np.mean(losses)}\")\n",
        "\n",
        "print(\"Time\",time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGMaccW3p1gp",
        "outputId": "397df6c0-8a6b-445a-afde-4616ae5cd242"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss - 0.22659756243228912\n",
            "Epoch 1: loss - 0.20030319690704346\n",
            "Epoch 2: loss - 0.1883440911769867\n",
            "Epoch 3: loss - 0.1798975169658661\n",
            "Epoch 4: loss - 0.17217399179935455\n",
            "Time 26.520280838012695\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate - Batch size of 32\n",
        "with torch.inference_mode():\n",
        "  # accuracy\n",
        "  accuracy = []\n",
        "\n",
        "  # test\n",
        "  clf.eval()\n",
        "  for X, y in test_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device).to(torch.float32).unsqueeze(dim=1)\n",
        "    y = y.to(device).to(torch.int64)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "    prediction = torch.argmax(y_logit, dim=1).tolist()\n",
        "\n",
        "    # get accuracy\n",
        "    accuracy.append(accuracy_score(y.tolist(), prediction))\n",
        "print(f\"Mean accuracy: {np.mean(accuracy)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNrBoFmjp_EP",
        "outputId": "d42beaea-68a8-4f24-a365-34e89b3b85d2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.954173322683706\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# With MNIST in-build data loader"
      ],
      "metadata": {
        "id": "_st3_sAhAS0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train - batch size of 1"
      ],
      "metadata": {
        "id": "9tnJFL8Wgyfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "# get as dataloaders\n",
        "train_data = MNIST(root='mnist_data/train', train=True, download=True, transform=transforms.ToTensor())\n",
        "test_data = MNIST(root='mnist_data/test', train=False, download=True, transform=transforms.ToTensor())\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data, batch_size=1)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=1)"
      ],
      "metadata": {
        "id": "5rXAATqHgWOT"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "import numpy as np\n",
        "\n",
        "# define the model\n",
        "clf = CNNClassifier().to(device)\n",
        "\n",
        "# loss and optimization\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=clf.parameters(), lr=0.01)\n",
        "\n",
        "start = time()\n",
        "losses = []\n",
        "\n",
        "# train loop\n",
        "for epoch in range(5):\n",
        "  # train\n",
        "  clf.train()\n",
        "  # iterate over the dataset\n",
        "  for X, y in train_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fn(y_logit, y)\n",
        "    losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    # update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # track loss\n",
        "  print(f\"Epoch {epoch}: loss - {np.mean(losses)}\")\n",
        "\n",
        "print(\"Time\",time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ehJ_Mjkgx2i",
        "outputId": "510d1695-338a-449e-a1aa-eaa87b8e403a"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss - 2.3101086616516113\n",
            "Epoch 1: loss - 2.3100993633270264\n",
            "Epoch 2: loss - 2.3100967407226562\n",
            "Epoch 3: loss - 2.3100950717926025\n",
            "Epoch 4: loss - 2.310094118118286\n",
            "Time 832.3194110393524\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate - Batch size of 1 only\n",
        "with torch.inference_mode():\n",
        "  # predictions\n",
        "  prediction = []\n",
        "\n",
        "  # test\n",
        "  clf.eval()\n",
        "  for X, y in test_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "    prediction.append(torch.argmax(y_logit, dim=1).item())\n",
        "\n",
        "# get accuracy\n",
        "acc = accuracy_score(test_data_y, prediction)\n",
        "print(f\"Mean accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GQUgnGSoY6D",
        "outputId": "4d7d37f5-c508-4aee-e929-48363ea30e52"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.0958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train - batch size of 32"
      ],
      "metadata": {
        "id": "hRLZqIaOlVs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(dataset=train_data, batch_size=32)\n",
        "test_dataloader = DataLoader(dataset=test_data, batch_size=32)"
      ],
      "metadata": {
        "id": "YkYYYq1Rlq67"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "# define the model\n",
        "clf = CNNClassifier().to(device)\n",
        "\n",
        "# loss and optimization\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(params=clf.parameters(), lr=0.01)\n",
        "\n",
        "start = time()\n",
        "losses = []\n",
        "\n",
        "# train loop\n",
        "for epoch in range(5):\n",
        "  # train\n",
        "  clf.train()\n",
        "  # iterate over the dataset\n",
        "  for X, y in train_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device)#.to(torch.float32).unsqueeze(dim=1)\n",
        "    y = y.to(device)#.to(torch.int64)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "\n",
        "    # loss\n",
        "    loss = loss_fn(y_logit, y)\n",
        "    losses.append(loss.cpu().detach().numpy())\n",
        "\n",
        "    # update\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  # track loss\n",
        "  print(f\"Epoch {epoch}: loss - {np.mean(losses)}\")\n",
        "\n",
        "print(\"Time\",time()-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y2kqsd1lOlZ",
        "outputId": "fb1cd000-53dc-4557-8e72-9b691c878e49"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: loss - 0.16718541085720062\n",
            "Epoch 1: loss - 0.1303280144929886\n",
            "Epoch 2: loss - 0.11642495542764664\n",
            "Epoch 3: loss - 0.10782646387815475\n",
            "Epoch 4: loss - 0.10267454385757446\n",
            "Time 61.4142370223999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate - Batch size of 32\n",
        "with torch.inference_mode():\n",
        "  # accuracy\n",
        "  accuracy = []\n",
        "\n",
        "  # test\n",
        "  clf.eval()\n",
        "  for X, y in test_dataloader:\n",
        "    # prepare the data\n",
        "    X = X.to(device)\n",
        "    y = y.to(device)\n",
        "\n",
        "    # predict\n",
        "    y_logit = clf(X)\n",
        "    prediction = torch.argmax(y_logit, dim=1).tolist()\n",
        "\n",
        "    # get accuracy\n",
        "    accuracy.append(accuracy_score(y.tolist(), prediction))\n",
        "print(f\"Mean accuracy: {np.mean(accuracy)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx4TEfcUhAcD",
        "outputId": "834e6fb4-f1d6-4f2e-a88e-b092c5625b4a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean accuracy: 0.965055910543131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion -> both the models with manual and in-build dataloaders are outputting the similar results. What is important is to use a batchsize > 1, otherwise the model is not learning."
      ],
      "metadata": {
        "id": "eVHpYqHWr-d4"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QvKDNuxrAtwv",
        "HfNYI7J72FmA",
        "sQ71AJJ22LD6",
        "P_LDQvALA7vu",
        "WbK4PBdrA860",
        "7VIEl4Bhp5IJ",
        "_st3_sAhAS0y",
        "9tnJFL8Wgyfs",
        "hRLZqIaOlVs4"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyN4kCI6sK5NiCdF8TG+ZEwk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}